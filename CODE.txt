file_list =  ['s3n://nyc-tlc/trip data/yellow_tripdata_2020-01.parquet',
              's3n://nyc-tlc/trip data/yellow_tripdata_2020-02.parquet',
              's3n://nyc-tlc/trip data/yellow_tripdata_2020-03.parquet',
		  's3n://nyc-tlc/trip data/yellow_tripdata_2020-04.parquet'
             ]


sdf = spark.read.parquet(*file_list)

sdf.printSchema()

sdf = sdf.withColumn("pickup_date", to_date(sdf.tpep_pickup_datetime))

sdf = sdf.withColumn("pickup_year_month", date_format(sdf.pickup_date,"yyyy-MM") )  # Will be string
sdf = sdf.withColumn("pickup_year", year(sdf.pickup_date) )
sdf = sdf.withColumn("pickup_month", month(sdf.pickup_date) )

sdf = sdf.withColumn("days_in_month", dayofmonth(last_day(sdf.pickup_date)) )

# Use a where Filter to only include records when pickup was on January 2020, 01st and later
sdf = sdf.where(sdf.pickup_date >= to_date(lit('2020-01-01')) )

# Check some data
sdf.select("tpep_pickup_datetime", "passenger_count", "trip_distance", "pickup_date", "pickup_year", 
"pickup_month", "pickup_year_month", "days_in_month").show()

# Calculate the total number of trips, passengers, distance etc. for the month
monthly_trip_count_sdf = sdf.groupby(sdf.pickup_year, sdf.pickup_month, sdf.days_in_month).count()
monthly_trip_count_sdf = monthly_trip_count_sdf.withColumnRenamed("count", "monthly_trip_count")

# Calculate average trip per month
monthly_trip_count_sdf = monthly_trip_count_sdf.withColumn("average_trips_per_month", monthly_trip_count_sdf.monthly_trip_count / monthly_trip_count_sdf.days_in_month)
monthly_trip_count_sdf.show()

# Generate daily counts
daily_trip_count_sdf = sdf.groupby(sdf.pickup_date, sdf.pickup_year, sdf.pickup_month).count()
daily_trip_count_sdf = daily_trip_count_sdf.withColumnRenamed("count", "daily_trip_count")

# Join the monthly data with the daily data 
trip_counts_sdf = daily_trip_count_sdf.join(monthly_trip_count_sdf, ["pickup_year", "pickup_month"])

# Check some data
trip_counts_sdf.show()

# Calculate indicator variable =1.0 if there are more daily trips than average
trip_counts_sdf = trip_counts_sdf.withColumn("more_trips_than_average", when(trip_counts_sdf.daily_trip_count > trip_counts_sdf.average_trips_per_month, 1.0).otherwise(0.0))



# Load in the weather_data_NYC_daily.csv file
weather_sdf = spark.read.csv('s3://4130projectdatasetbucket/weather_data_NYC_daily.csv', header=True, inferSchema=True)

weather_sdf = weather_sdf.drop('Station','latitude','longitude','Borough','City','State','ZipCode')
weather_sdf.printSchema()

# Convert WDate into an actual Date column
weather_sdf = weather_sdf.withColumn('WDate', to_date(weather_sdf.WDate, 'M/d/yyyy'))
weather_sdf.show()

# Create year, month columns
weather_sdf = weather_sdf.withColumn("weather_year", year(weather_sdf.WDate) )
weather_sdf = weather_sdf.withColumn("weather_month", month(weather_sdf.WDate) )

weather_sdf.printSchema()

# Get the monthly average temperature
monthly_weather_sdf = weather_sdf.groupby(weather_sdf.weather_year, weather_sdf.weather_month).agg(mean("Temperature_Avg").alias("monthly_average_temperature"))
monthly_weather_sdf.show() 

# Join the average back in with the daily data
weather_sdf = weather_sdf.join(monthly_weather_sdf, ["weather_year", "weather_month"])
weather_sdf.show()

# Create an indicator variable if the daily average is higher (1) or lower (0) than the monthly average
weather_sdf = weather_sdf.withColumn("higher_temperature_than_average", when(weather_sdf.Temperature_Avg > weather_sdf.monthly_average_temperature, 1.0).otherwise(0.0))


# Joining the tlc data and weather data by matching the date columns
joined_counts = trip_counts_sdf.join(weather_sdf,
               trip_counts_sdf.pickup_date == weather_sdf.WDate,
               "inner")


# Creating last dataframe to use to create model
cleaned_joined_counts = joined_counts.select("pickup_date", "daily_trip_count", "average_trips_per_month", 
"more_trips_than_average", "Temperature_Avg", "Dewpot_Avg", "Humidity_Avg", "Wdspeed_Avg", "Precipitation_Total", 
"monthly_average_temperature", "higher_temperature_than_average")
cleaned_joined_counts.show()


# Feature Engineering & Model Building
 
from pyspark.sql.functions import *
from pyspark.sql.types import IntegerType, DateType
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler
from pyspark.ml import Pipeline
# Import the logistic regression model
from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel
# Import the evaluation module
from pyspark.ml.evaluation import *
# Import the model tuning module
from pyspark.ml.tuning import *
import numpy as np
 
 
# Create a label. =1 if soda, =0 otherwise
cleaned_joined_counts = cleaned_joined_counts.withColumn("label", when(cleaned_joined_counts.more_trips_than_average == 1.0, 1.0).otherwise(0.0) )
 
# Creating vector assembling for all double columns as features 
assembler = VectorAssembler(inputCols=['Precipitation_Total', 'Dewpot_Avg', 'Wdspeed_Avg', 'Temperature_Avg', 'Humidity_Avg', 'monthly_average_temperature', 'higher_temperature_than_average'], outputCol="features")

# MinMax Scale the data 
scaler = MinMaxScaler(min=0., max=1., inputCol='features', outputCol='features_scaled')

# Creating the pipeline, only needing to use the vector assembler
temp_pipe = Pipeline(stages=[assembler,scaler])

# Fit the pipeline to the joined spark dataframe
transformed_sdf = temp_pipe.fit(cleaned_joined_counts).transform(cleaned_joined_counts)

# Review the transformed features
transformed_sdf.select('pickup_date','daily_trip_count','more_trips_than_average', 'Precipitation_Total', 'Dewpot_Avg', 'Wdspeed_Avg', 'Temperature_Avg', 'Humidity_Avg', 'monthly_average_temperature','higher_temperature_than_average', 'label', 'features', 'features_scaled').show(20, truncate=False)
 
# Split the data into training and testing sets
trainingData, testData = transformed_sdf.randomSplit([0.7, 0.3], seed=4281)
 
# Create a Linear Regression Estimator
lr = LogisticRegression()
 
# Fit the model to the training data
model = lr.fit(trainingData)
 
# Show model coefficients and intercept
print("Coefficients: ", model.coefficients)
print("Intercept: ", model.intercept)
 
# Test the model on the testData
test_results = model.transform(testData)

# Show the test results
test_results.select('pickup_date', 'more_trips_than_average', 'higher_temperature_than_average', 'rawPrediction', 
'probability', 'prediction', 'label').show(truncate=False)
 
# Show the confusion matrix
test_results.groupby('label').pivot('prediction').count().show()

### Cross Validation and Grid Search Model Optimization

# Create a BinaryClassificationEvaluator to evaluate how well the model works
evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")

# Create the parameter grid (empty for now)
grid = ParamGridBuilder().build()

# Create the CrossValidator
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3 )

# Use the CrossValidator to Fit the training data
cv = cv.fit(trainingData)

# Show the average performance over the three folds
cv.avgMetrics

# Evaluate the test data using the cross-validator model
evaluator.evaluate(cv.transform(testData))

## Parameter Grid

# Create a grid to hold hyperparameters 
grid = ParamGridBuilder()
grid = grid.addGrid(lr.regParam, [0.0, 0.5, 1.0] )
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

# Build the grid
grid = grid.build()
print('Number of models to be tested: ', len(grid))

# Create the CrossValidator using the new hyperparameter grid
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)

# Call cv.fit() to create models with all of the combinations of parameters in the grid
all_models = cv.fit(trainingData)

print("Average Metrics for Each model: ", all_models.avgMetrics)

# Gather the metrics and parameters of the model with the best average metrics
hyperparams = all_models.getEstimatorParamMaps()[np.argmax(all_models.avgMetrics)]

# Print out the list of hyperparameters for the best model
for i in range(len(hyperparams.items())):
	print([x for x in hyperparams.items()][i])


# Choose the best model
bestModel = all_models.bestModel
print("Area under ROC curve:", bestModel.summary.areaUnderROC)

# Use the model 'bestModel' to predict the test set
test_results = bestModel.transform(testData)

# Show the test results
test_results.select('pickup_date', 'more_trips_than_average', 'higher_temperature_than_average', 'probability', 
'prediction', 'label').show(truncate=False)

# Evaluate the predictions. Area Under ROC curve
print(evaluator.evaluate(test_results))


# Show the confusion matrix again
test_results.groupby('label').pivot('prediction').count().show()

# Save the best model 
model_path = "s3://4130projectdatasetbucket/tlc_temp_logistic_regression_model"
bestModel.write().overwrite().save(model_path)

# Used for loading up the model to use with new data
model_path = "s3://4130projectdatasetbucket/tlc_temp_logistic_regression_model"
# Load up the model from disk. Note the use of LogisticRegressionModel
myModel = LogisticRegressionModel.load(model_path)

# Visualization Part

### Use pip3 install for s3fs, boto3, pandas, matplotlib, seaborn
import io
import pandas as pd
import s3fs
import boto3
import matplotlib.pyplot as plt
import seaborn as sns
# Import some modules we will need later on
from pyspark.sql.functions import col, isnan, when, count, udf, to_date, year, month, date_format, size, split
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

### Correlation Matrix

import seaborn as sns
# Convert the numeric values to vector columns
vector_column = "correlation_features"
numeric_columns = ['Precipitation_Total', 'Dewpot_Avg', 'Wdspeed_Avg', 'Temperature_Avg', 'Humidity_Avg', 'monthly_average_temperature', 'higher_temperature_than_average']
assembler = VectorAssembler(inputCols=numeric_columns, outputCol=vector_column)
sdf_vector = assembler.transform(test_results).select(vector_column)

# Create the correlation matrix, then get just the values and convert to a list
matrix = Correlation.corr(sdf_vector, vector_column).collect()[0][0]
correlation_matrix = matrix.toArray().tolist() 
# Convert the correlation to a Pandas dataframe
correlation_matrix_df = pd.DataFrame(data=correlation_matrix, columns=numeric_columns, 
index=numeric_columns) 

plt.figure(figsize=(16,5)) 
sns.heatmap(correlation_matrix_df, 
		xticklabels=correlation_matrix_df.columns.values,
		yticklabels=correlation_matrix_df.columns.values, cmap="Greens", annot=True)
plt.savefig("correlation_matrix.png")


### Saving correlation plot to S3

import io
import matplotlib.pyplot as plt
import s3fs 
# Create a plot
plt.plot("correlation_matrix.png")
# Create a buffer to hold the figure
img_data = io.BytesIO()
# Write the figure to the buffer
plt.savefig(img_data, format='png', bbox_inches='tight')
img_data.seek(0)

# Connect to the s3fs file system
s3 = s3fs.S3FileSystem(anon=False)
with s3.open('s3://4130projectdatasetbucket/correlation_matrix.png', 'wb') as f:
	f.write(img_data.getbuffer())